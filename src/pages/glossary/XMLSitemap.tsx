import GlossaryTermPage from '@/components/GlossaryTermPage';

const XMLSitemapGlossary = () => (
  <GlossaryTermPage
    slug="xml-sitemap"
    term="XML Sitemap"
    category="technical-seo"
    definition="An XML sitemap is a file that lists all important URLs on your website, helping search engines discover, crawl, and index your content more efficiently. This machine-readable document provides search engine crawlers with a comprehensive map of your site's structure, including metadata about when pages were last updated and their relative importance. XML sitemaps are essential for technical SEO, particularly for large websites, new sites, or those with complex navigation structures."
    content={{
      introduction: `XML sitemaps serve as roadmaps for search engine crawlers, listing the pages you want indexed along with metadata about each URL. While search engines can discover pages through links, sitemaps ensure no important content is missed, especially on large sites or those with complex architectures. Think of an XML sitemap as a table of contents for search engines—it tells them exactly what pages exist, when they were updated, and how important they are relative to other pages. For websites with thousands of pages, proper sitemap implementation can mean the difference between content being discovered in days versus months.`,
      whyItMatters: `XML sitemaps are crucial for ensuring search engines can find and index all your important content. New websites especially benefit because they lack the backlink profile needed for comprehensive crawling. Sites with complex JavaScript rendering, orphan pages, or deep navigation hierarchies may have content that's difficult for crawlers to discover naturally. Sitemaps also communicate update frequency, helping search engines prioritize recrawling recently changed content. For large e-commerce sites with thousands of products, sitemaps ensure new items get indexed quickly. Without a sitemap, you're relying entirely on internal linking and external backlinks for discovery, which can leave valuable pages unindexed.`,
      howItWorks: `XML sitemaps follow a standardized protocol that search engines understand. The file contains a list of URLs wrapped in XML tags, with optional metadata for each URL. When you submit a sitemap to Google Search Console or reference it in your robots.txt file, search engine crawlers fetch and parse the file. They then add the URLs to their crawl queue, prioritizing based on the priority and changefreq hints you provide. Sitemaps don't guarantee indexing—they simply ensure discovery. Google still evaluates each page for quality before deciding whether to include it in search results. Large sites can use sitemap index files to organize multiple sitemaps, with each individual sitemap containing up to 50,000 URLs.`,
      sections: [
        { id: 'structure', title: 'Sitemap Structure', content: `XML sitemaps contain: URL location (loc) - the full page address including protocol. Last modified (lastmod) - timestamp of when content last changed. Change frequency (changefreq) - hints like daily, weekly, monthly for how often content updates. Priority (priority) - relative importance from 0.0 to 1.0 compared to other pages on your site. Only loc is required; the others are optional hints to crawlers that may influence crawl scheduling but don't guarantee any specific behavior.` },
        { id: 'benefits', title: 'Benefits of Sitemaps', content: `Sitemaps help with: Faster discovery of new content—especially critical for news sites and e-commerce. Indexing orphan pages that have few internal links pointing to them. Communicating content priorities to help crawlers focus on important pages. Helping large sites with 10,000+ pages get comprehensively crawled. Indexing dynamic content generated by JavaScript or database queries. Providing metadata about page relationships and update frequencies.` },
        { id: 'best-practices', title: 'Best Practices', content: `Optimize your sitemap by: Including only indexable, canonical URLs—never include noindex pages or non-canonical versions. Keeping under 50,000 URLs per sitemap file and under 50MB uncompressed. Using sitemap index files to organize multiple sitemaps for large sites. Updating lastmod only when content actually changes to maintain credibility with crawlers. Submitting to Google Search Console for monitoring and error detection. Referencing your sitemap in robots.txt with Sitemap: directive.` },
        { id: 'types', title: 'Sitemap Types', content: `Different sitemap varieties serve specific purposes: Standard XML sitemap for regular web pages and blog posts. Image sitemap for helping image content appear in Google Images. Video sitemap for video content with metadata like duration, thumbnail, and description. News sitemap for Google News publishers with publication dates and article names. Sitemap index files that reference multiple sitemaps for organization. Each type uses specific XML tags to provide relevant metadata to search engines.` }
      ],
      bestPractices: [
        'Only include canonical, indexable URLs—exclude pages with noindex tags, redirects, or non-200 status codes',
        'Keep each sitemap under 50,000 URLs and 50MB; use sitemap index files for larger sites',
        'Update lastmod dates only when content genuinely changes to maintain crawler trust',
        'Submit your sitemap through Google Search Console to monitor indexing status and errors',
        'Generate sitemaps dynamically for sites with frequently changing content rather than maintaining static files'
      ],
      commonMistakes: [
        'Including non-canonical URLs, redirecting pages, or noindex pages that waste crawl budget and confuse search engines',
        'Setting lastmod dates to update automatically regardless of actual content changes, which reduces crawler trust',
        'Forgetting to update the sitemap when adding new pages or removing old ones, leaving content undiscovered or pointing to 404 errors'
      ],
      example: `An e-commerce site with 15,000 products was experiencing slow indexing of new items—sometimes taking 3-4 weeks for new products to appear in Google. They implemented a dynamic XML sitemap system that automatically added new products with accurate lastmod timestamps and submitted the sitemap to Search Console. They also created separate sitemaps for products, categories, and blog content for better organization. After implementation, new products began appearing in Google within 2-5 days. The sitemap also helped them identify 200+ orphan product pages that had no internal links, which they then properly integrated into category navigation.`
    }}
    tableOfContents={[
      { id: 'introduction', title: 'Introduction' },
      { id: 'why-it-matters', title: 'Why It Matters' },
      { id: 'how-it-works', title: 'How It Works' },
      { id: 'structure', title: 'Sitemap Structure' },
      { id: 'benefits', title: 'Benefits of Sitemaps' },
      { id: 'best-practices', title: 'Best Practices' },
      { id: 'types', title: 'Sitemap Types' },
      { id: 'data-insights', title: 'Data Insights' },
      { id: 'related-terms', title: 'Related Terms' }
    ]}
    chartData={{ type: 'bar', title: 'Sitemap Impact on Indexing', data: [{ name: 'With Sitemap', value: 85 }, { name: 'Without Sitemap', value: 60 }] }}
    imageUrl="https://images.unsplash.com/photo-1551288049-bebda4e38f71?w=800&h=400&fit=crop"
    imageAlt="XML sitemap file structure showing website URL mapping for search engine crawling"
  />
);

export default XMLSitemapGlossary;